{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import *\n",
    "import glob\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#torch.manual_seed(1)\n",
    "\n",
    "\"\"\"\n",
    "This function extract the features from the MIDI files.\n",
    "\n",
    "Input : Directory containing the midi files\n",
    "outputs : numpy ndarray containing numpy arrays of the concatenated elements of the MIDI files.\n",
    "          Elements are feature extracted from the MIDI files.\n",
    "\"\"\"\n",
    "def read_midi_dataset(file):\n",
    "    data = list()\n",
    "    for midi in glob.glob(file):\n",
    "        mu = converter.parse(midi)\n",
    "        s2 = instrument.partitionByInstrument(mu)\n",
    "        # parts[0] means we only takes into account piano\n",
    "        note2parse = s2.parts[0].recurse() \n",
    "        temp = list()\n",
    "        for note_ in note2parse:\n",
    "            if isinstance(note_, note.Note): # isinstance check if element is a note\n",
    "                temp.append(str(note_.pitch))\n",
    "            elif isinstance(note_, chord.Chord): # check if it is a chord\n",
    "                temp.append('.'.join(str(n) for n in note_.normalOrder))\n",
    "                \n",
    "        data.append(temp)\n",
    "        \n",
    "    data = np.array(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function transforms a numpy ndarray containaing arrays of elements of MIDI files into one list of\n",
    "these elements. Example : [[a,b][c,d]] => [a,b,c,d]\n",
    "\"\"\"\n",
    "def from_ndarrays_to_list(data):\n",
    "    return [note for notes_ in data for note in notes_] \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function deletes from the dataset elements that do not appear more than a particular frequency.\n",
    "It is a filter.\n",
    "Input : numpy ndarray containing numpy arrays of the concatenated elements of the MIDI files.\n",
    "Output : List of list. Each list is a concatenation of all the elements of a MIDI file.\n",
    "\"\"\"\n",
    "def get_vocabulary(data):\n",
    "    data_ = from_ndarrays_to_list(data)\n",
    "    # frequence of notes\n",
    "    freq = dict(Counter(data_))\n",
    "    # unique_elements is the sorted set of unique elements of the set of MIDI files. The elements selected depends\n",
    "    # on a particular frequency. Therefore, it is the total vacabulary of the dataset.\n",
    "    unique_note = sorted([note_ for note_, count in freq.items()])\n",
    "    \n",
    "    vocab_dict = {}\n",
    "    for i in range(len(unique_note)): vocab_dict[unique_note[i]] = i\n",
    "        \n",
    "    return unique_note, vocab_dict\n",
    "\n",
    "\n",
    "def note_to_vec(vocab, data, size_vocab, dim_embed):\n",
    "    \n",
    "    embeds = nn.Embedding(size_vocab, dim_embed)\n",
    "    \n",
    "    data_embed = list()\n",
    "    for song in data:\n",
    "        data_embed.append(np.array([embeds(torch.tensor([vocab[note]],\n",
    "                                                        dtype=torch.long)).detach().numpy()[0] for note in song]))\n",
    "\n",
    "    return data_embed\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "def note_to_ind(vocab, data):\n",
    "    note2ind = {u:i for i, u in enumerate(vocab)}\n",
    "    ind2note = np.array(vocab)\n",
    "    \n",
    "    dataInd = list()\n",
    "    for song in data: dataInd.append(np.array([note2ind[note] for note in song]))\n",
    "        \n",
    "    ind2note = {}\n",
    "    for note, ind in note2ind.items():\n",
    "        ind2note[ind] = note\n",
    "    \n",
    "    return dataInd, note2ind, ind2note\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function creates the X and y matrices needed by the model.\n",
    "We use a sliding window mechanism in order to create this dataset.\n",
    "[a,b,c,d,e,f,g] becomes x1=[a,b,c], y1=[d] then x2=[b,c,d], y2=[e] etc.\n",
    "\n",
    "Input : List of list. Each list is a concatenation of all the elements of a MIDI file.\n",
    "Output : matrix X and vector y.\n",
    "\"\"\"\n",
    "def training_target_samples(data_embed, dataInd, window_size, show_example=False): #time_step = window\n",
    "    x = list()\n",
    "    y = list()\n",
    "\n",
    "    \"\"\"\n",
    "    for notes_ in data:\n",
    "        for i in range(len(notes_) - window_size):\n",
    "            x.append(notes_[i : i + window_size])\n",
    "            y.append(notes_[i + window_size])\n",
    "    \"\"\"\n",
    "            \n",
    "    for i in range(len(data_embed)):\n",
    "        for j in range(len(dataInd[i]) - window_size):\n",
    "            x.append(data_embed[i][j : j + window_size])\n",
    "            y.append(dataInd[i][j + window_size])\n",
    "            \n",
    "    if show_example is True:\n",
    "        for i, (trainingInd, targetInd) in enumerate(zip(training[:5], target[:5])):\n",
    "            print(\"Step {:4d}\".format(i))\n",
    "            print(\"  Input: {} ({:s})\".format(trainingInd, repr(ind2note[trainingInd])))\n",
    "            print(\"  expected output: {} ({:s})\".format(targetInd, repr(ind2note[targetInd])))\n",
    "    \n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "def split_reshape(X, y, split_ratio, size_vocab, dim_embed_x, dim_embed_y):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(np.array(X_dataset), np.array(y_dataset),\n",
    "                                                        test_size=split_ratio, random_state=0)\n",
    "        \n",
    "    X_train, y_train = reshape_datasets(X_train, np.array(y_train), size_vocab, dim_embed_x, dim_embed_y)\n",
    "    X_test, y_test = reshape_datasets(X_test, np.array(y_test), size_vocab, dim_embed_x, dim_embed_y)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def reshape_datasets(X, y, size_vocab, dim_embed_x, dim_embed_y):\n",
    "    #y_train = np.eye(size_vocab)[y_train]\n",
    "    #y_test = np.eye(size_vocab)[y_test]\n",
    "    \n",
    "    # batch_size , sequence_length , size_encoding = 1 (> 1 if one-hot encoding)\n",
    "    \n",
    "    nb_samples = X.shape[0]\n",
    "    seq_length = X.shape[1]\n",
    "    X = np.reshape(X, (nb_samples, seq_length, dim_embed_x))/float(size_vocab) # Normalization\n",
    "    y = np.reshape(y, (nb_samples, dim_embed_y))\n",
    "\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def ind_to_embedding(dataInd, dataEmbed):\n",
    "    ind2embed = {}\n",
    "    for i in range(len(dataInd)): # -1270\n",
    "        ind2embed[dataInd[i]] = dataEmbed[i]\n",
    "    \n",
    "    return ind2embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"/home/cj/Bureau/Master2/Q2/deep_learning/project/tf_dataset/*.mid\"\n",
    "data = read_midi_dataset(file)\n",
    "unique_note, vocab = get_vocabulary(data)\n",
    "size_vocab = len(unique_note)\n",
    "\n",
    "dim_x = 4\n",
    "data_embed = note_to_vec(vocab, data, size_vocab, dim_x)\n",
    "dataInd, note2ind, ind2note = note_to_ind(unique_note, data)\n",
    "ind2embed = ind_to_embedding(dataInd[0], data_embed[0])\n",
    "\n",
    "window_size = 5\n",
    "X_dataset, y_dataset = training_target_samples(data_embed, dataInd, window_size)\n",
    "\n",
    "split_ratio = 0.99\n",
    "dim_y = 1\n",
    "X_train, X_test, y_train, y_test = split_reshape(X_dataset, y_dataset, split_ratio,\n",
    "                                                 size_vocab, dim_x, dim_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "test_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
    "\n",
    "batch_size = 3\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=1, drop_last=True) #batch_size=1 for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for x, y in train_loader:\n",
    "    #print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lstm_model(\n",
       "  (lstm1): LSTM(4, 12, batch_first=True)\n",
       "  (lstm2): LSTM(12, 12)\n",
       "  (lstm3): LSTM(12, 12)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fully_connected): Linear(in_features=12, out_features=111, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class lstm_model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers, dropout):\n",
    "        super(lstm_model, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #Defining the layers\n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim, n_layers)\n",
    "        self.lstm3 = nn.LSTM(hidden_dim, hidden_dim, n_layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fully_connected = nn.Linear(hidden_dim, output_size)\n",
    "      \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        len_seq = x.size(1)\n",
    "        x = x.to(dtype=torch.float64)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        h_t = hidden[0].to(dtype=torch.float64)\n",
    "        c_t = hidden[1].to(dtype=torch.float64)\n",
    "                \n",
    "        out, (h_t, c_t) = self.lstm1(x, (h_t, c_t))\n",
    "        h_t = self.dropout(h_t)\n",
    "        out, (h_t, c_t) = self.lstm2(h_t, (h_t, c_t)) \n",
    "        h_t = self.dropout(h_t)\n",
    "        out, (h_t, c_t) = self.lstm3(h_t, (h_t, c_t)) \n",
    "     \n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fully_connected(out)\n",
    " \n",
    "        return out\n",
    "\n",
    "    \"\"\"\n",
    "    This function can be used to extract the output of the last predictions of each batch\n",
    "    \"\"\"\n",
    "    def last_output(batc_size, out):\n",
    "        lengths = [len_seq for i in range(batch_size)]\n",
    "        idx = (torch.LongTensor(lengths) - 1).view(-1, 1)\n",
    "        idx = idx.expand(len(lengths), out.size(2))\n",
    "        time_dimension = 1 # because batch_first is True so the time step is dimension 1 !\n",
    "        idx = idx.unsqueeze(time_dimension)\n",
    "        \n",
    "        if out.is_cuda:\n",
    "            idx = idx.cuda(out.data.get_device())\n",
    "        \n",
    "        out = out.gather(time_dimension, idx).squeeze(time_dimension)\n",
    "        \n",
    "        return out\n",
    " \n",
    "      \n",
    "    \"\"\"\n",
    "    Generates the hidden state and the cell state used in a lstm layers\n",
    "    \"\"\"\n",
    "    def init_hidden(self,  batch_size):\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return (torch.zeros(self.n_layers, batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.n_layers, batch_size, self.hidden_dim))\n",
    "\n",
    "# Instantiate the model with hyperparameters\n",
    "input_size = dim_x # because no one-hot encoder or embedding layer or anything like that\n",
    "output_size = size_vocab\n",
    "hidden_dim = 12\n",
    "n_layers = 1\n",
    "dropout = 0.3\n",
    "model = lstm_model(input_size, output_size, hidden_dim, n_layers, dropout)\n",
    "model = model.to(dtype=torch.float64)\n",
    "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class attention_model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers, dropout):\n",
    "        super(attention_model, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #Defining the layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fully_connected = nn.Linear(hidden_dim, output_size)\n",
    "      \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        len_seq = x.size(1)\n",
    "        x = x.to(dtype=torch.float64)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        h_t = hidden[0].to(dtype=torch.float64)\n",
    "        c_t = hidden[1].to(dtype=torch.float64)\n",
    "                \n",
    "        out, (h_t, c_t) = self.lstm1(x, (h_t, c_t))\n",
    "        h_t = self.dropout(h_t)\n",
    "        out, (h_t, c_t) = self.lstm2(h_t, (h_t, c_t)) \n",
    "        h_t = self.dropout(h_t)\n",
    "        out, (h_t, c_t) = self.lstm3(h_t, (h_t, c_t)) \n",
    "     \n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fully_connected(out)\n",
    " \n",
    "        return out\n",
    "\n",
    "    \"\"\"\n",
    "    This function can be used to extract the output of the last predictions of each batch\n",
    "    \"\"\"\n",
    "    def last_output(batc_size, out):\n",
    "        lengths = [len_seq for i in range(batch_size)]\n",
    "        idx = (torch.LongTensor(lengths) - 1).view(-1, 1)\n",
    "        idx = idx.expand(len(lengths), out.size(2))\n",
    "        time_dimension = 1 # because batch_first is True so the time step is dimension 1 !\n",
    "        idx = idx.unsqueeze(time_dimension)\n",
    "        \n",
    "        if out.is_cuda:\n",
    "            idx = idx.cuda(out.data.get_device())\n",
    "        \n",
    "        out = out.gather(time_dimension, idx).squeeze(time_dimension)\n",
    "        \n",
    "        return out\n",
    " \n",
    "      \n",
    "    \"\"\"\n",
    "    Generates the hidden state and the cell state used in a lstm layers\n",
    "    \"\"\"\n",
    "    def init_hidden(self,  batch_size):\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return (torch.zeros(self.n_layers, batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.n_layers, batch_size, self.hidden_dim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1... Training Loss: 4.704359... Validation Loss Loss: 4.681416\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters for the training\n",
    "\n",
    "print_every = batch_size\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "n_epochs = 1\n",
    "lr=0.01\n",
    "\n",
    "# Define Loss, Optimizer, accuracy\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "train_accuracy = list()\n",
    "test_accuracy = list()\n",
    "\n",
    "save = 0\n",
    "\n",
    "# Training Run\n",
    "#model.to(dtype=torch.float64).train()\n",
    "model.train()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
    "    losses = []\n",
    "    true_pred_train = 0\n",
    "    true_pred_test = 0\n",
    "    i = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        i+=1\n",
    "        # Forward\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        pred_train = model(inputs)\n",
    "        \n",
    "        # Compute Loss and backpropagation\n",
    "        loss = criterion(pred_train, targets.view(-1).long())\n",
    "        losses.append(loss.item())\n",
    "        loss.backward() # Does backpropagation and calculates gradients\n",
    "        optimizer.step() # Updates the weights accordingly\n",
    "        \n",
    "        # Check if the right target has been predicted for the last input of the batch\n",
    "        prob = nn.functional.softmax(pred_train[-1], dim=0).data \n",
    "        note_ind = torch.max(prob, dim=0)[1].item() # [1] take indice\n",
    "       \n",
    "        if note_ind == targets[-1].item(): # compare to the last target of the batch\n",
    "            true_pred_train+=1\n",
    "        \n",
    "    # mean accuracy of this epoch\n",
    "    train_accuracy.append(true_pred_train/len(train_loader.dataset))\n",
    "    \n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    for inputs, target in test_loader:\n",
    "\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        pred_test = model(inputs)\n",
    "        \n",
    "        val_loss = criterion(pred_test, target.view(-1).long())\n",
    "        val_losses.append(val_loss.item())\n",
    "        \n",
    "        prob = nn.functional.softmax(pred_test[-1], dim=0).data \n",
    "        note_ind = torch.max(prob, dim=0)[1].item() # [1] take indice\n",
    "        \n",
    "        if note_ind == target[-1].item():\n",
    "            true_pred_test+=1\n",
    "    \n",
    "    test_accuracy.append(true_pred_test/len(test_loader.dataset))\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    print(\"Epoch: {}/{}...\".format(epoch, n_epochs),\n",
    "\n",
    "        \"Training Loss: {:.6f}...\".format(np.mean(losses)),\n",
    "        \"Validation Loss Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "    \n",
    "    save+=1\n",
    "    if save == 15:\n",
    "        save = 0\n",
    "        if np.mean(val_losses) <= valid_loss_min:\n",
    "            valid_loss_min = np.mean(val_losses)\n",
    "            torch.save(model.state_dict(), './state_dict.pt')\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}). Saving model ...'.\n",
    "                  format(valid_loss_min,np.mean(val_losses)))\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_generation(dataInd, window_size, ind2note, ind2embed):\n",
    "    song_ind = np.random.randint(0, len(dataInd))\n",
    "    note_ind = np.random.randint(0+window_size, len(dataInd[song_ind])-window_size-1)\n",
    "    init_sequence = dataInd[song_ind][note_ind:note_ind+window_size]\n",
    "    \n",
    "    music_generated = list()\n",
    "    input_sequence = list()\n",
    "    for ind in init_sequence:\n",
    "        note = ind2note[ind]\n",
    "        embed = ind2embed[ind]\n",
    "        input_sequence.append(embed)\n",
    "        music_generated.append(note)\n",
    "        \n",
    "        \n",
    "    return music_generated, np.array(input_sequence)\n",
    "\n",
    "def music_generation(input_seq):\n",
    "    input_seq = np.reshape(input_seq, (1, input_seq.shape[0], dim_x))\n",
    "    input_seq = torch.from_numpy(input_seq)\n",
    "    input_seq.to(device)\n",
    "    \n",
    "    pred = model(input_seq)\n",
    "    prob = nn.functional.softmax(pred_test[-1], dim=0).data \n",
    "    note_ind = torch.max(prob, dim=0)[1].item() # [1] take indice\n",
    "    \n",
    "    \n",
    "    return note_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_generated, input_sequence = initialise_generation(dataInd, window_size, ind2note, ind2embed)\n",
    "\n",
    "nb_steps = 5\n",
    "for i in range(nb_steps):\n",
    "    pred_ind = music_generation(input_sequence)\n",
    "    pred_note = ind2note[pred_ind]\n",
    "    music_generated.append(pred_note)\n",
    "    pred_embed = ind2embed[pred_ind]\n",
    "    input_sequence = np.append(np.delete(input_sequence, 0, axis=0), [pred_embed], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('rstudio': conda)",
   "language": "python",
   "name": "python37464bitrstudioconda85139be567af4fb89143852fd1a4ee46"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
