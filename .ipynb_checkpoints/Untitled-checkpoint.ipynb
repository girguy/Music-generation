{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Epoch: 1/1 =>  Train Loss: 4.582185,  Val Loss: 4.377211,  Train accuracy: 0.023622, Test accuracy: 0.074236\n",
      "pytorch_song.mid downloaded succesfully !\n"
     ]
    }
   ],
   "source": [
    "from music21 import *\n",
    "import glob\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function extract the features from the MIDI files.\n",
    "\n",
    "Input : Directory containing the midi files\n",
    "outputs : numpy ndarray containing numpy arrays of the concatenated elements of the MIDI files.\n",
    "          Elements are feature extracted from the MIDI files.\n",
    "\"\"\"\n",
    "def read_midi_dataset(file):\n",
    "    data = list()\n",
    "    for midi in glob.glob(file):\n",
    "        mu = converter.parse(midi)\n",
    "        s2 = instrument.partitionByInstrument(mu)\n",
    "        # parts[0] means we only takes into account piano\n",
    "        note2parse = s2.parts[0].recurse() \n",
    "        temp = list()\n",
    "        for note_ in note2parse:\n",
    "            if isinstance(note_, note.Note): # isinstance check if element is a note\n",
    "                temp.append(str(note_.pitch))\n",
    "            elif isinstance(note_, chord.Chord): # check if it is a chord\n",
    "                temp.append('.'.join(str(n) for n in note_.normalOrder))\n",
    "                \n",
    "        data.append(temp)\n",
    "        \n",
    "    data = np.array(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function transforms a numpy ndarray containaing arrays of elements of MIDI files into one list of\n",
    "these elements. Example : [[a,b][c,d]] => [a,b,c,d]\n",
    "\"\"\"\n",
    "def from_ndarrays_to_list(data):\n",
    "    return [note for notes_ in data for note in notes_] \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function deletes from the dataset elements that do not appear more than a particular frequency.\n",
    "It is a filter.\n",
    "Input : numpy ndarray containing numpy arrays of the concatenated elements of the MIDI files.\n",
    "Output : List of list. Each list is a concatenation of all the elements of a MIDI file.\n",
    "\"\"\"\n",
    "def get_vocabulary(data):\n",
    "    data_ = from_ndarrays_to_list(data)\n",
    "    # frequence of notes\n",
    "    freq = dict(Counter(data_))\n",
    "    # unique_elements is the sorted set of unique elements of the set of MIDI files.\n",
    "    # The elements selected depends on a particular frequency.\n",
    "    # Therefore, it is the total vacabulary of the dataset.\n",
    "    unique_note = sorted([note_ for note_, count in freq.items()])\n",
    "    \n",
    "    vocab_dict = {}\n",
    "    for i in range(len(unique_note)): vocab_dict[unique_note[i]] = i\n",
    "        \n",
    "    return unique_note, vocab_dict\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "def note_to_vec(vocab, data, size_vocab, dim_input):\n",
    "    \n",
    "    embeds = nn.Embedding(size_vocab, dim_input)\n",
    "    \n",
    "    data_embed = list()\n",
    "    for song in data:\n",
    "        data_embed.append(np.array(\n",
    "            [embeds(torch.tensor([vocab[note]], dtype=torch.long)).detach().numpy()[0] for note in song]))\n",
    "\n",
    "    return data_embed\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "def note_to_ind(vocab, data):\n",
    "    note2ind = {u:i for i, u in enumerate(vocab)}\n",
    "    ind2note = np.array(vocab)\n",
    "    \n",
    "    dataInd = list()\n",
    "    for song in data: dataInd.append(np.array([note2ind[note] for note in song]))\n",
    "        \n",
    "    ind2note = {}\n",
    "    for note, ind in note2ind.items():\n",
    "        ind2note[ind] = note\n",
    "    \n",
    "    return dataInd, note2ind, ind2note\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function creates the X and y matrices needed by the model.\n",
    "We use a sliding window mechanism in order to create this dataset.\n",
    "[a,b,c,d,e,f,g] becomes x1=[a,b,c], y1=[d] then x2=[b,c,d], y2=[e] etc.\n",
    "\n",
    "Input : List of list. Each list is a concatenation of all the elements of a MIDI file.\n",
    "Output : matrix X and vector y.\n",
    "\"\"\"\n",
    "def training_target_samples(data_embed, dataInd, window_size, show_example=False): #time_step = window\n",
    "    x = list()\n",
    "    y = list()\n",
    "\n",
    "    for i in range(len(data_embed)):\n",
    "        for j in range(len(dataInd[i]) - window_size):\n",
    "            x.append(data_embed[i][j : j + window_size])\n",
    "            y.append(dataInd[i][j + window_size])\n",
    "            \n",
    "    if show_example is True:\n",
    "        for i, (trainingInd, targetInd) in enumerate(zip(training[:5], target[:5])):\n",
    "            print(\"Step {:4d}\".format(i))\n",
    "            print(\"  Input: {} ({:s})\".format(trainingInd, repr(ind2note[trainingInd])))\n",
    "            print(\"  expected output: {} ({:s})\".format(targetInd, repr(ind2note[targetInd])))\n",
    "    \n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "def split_reshape(X, y, split_ratio, size_vocab, dim_input, dim_output):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(np.array(X), np.array(y),\n",
    "                                                        test_size=split_ratio, random_state=0)\n",
    "        \n",
    "    X_train, y_train = reshape_datasets(X_train, np.array(y_train), size_vocab, dim_input, dim_output)\n",
    "    X_test, y_test = reshape_datasets(X_test, np.array(y_test), size_vocab, dim_input, dim_output)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "def reshape_datasets(X, y, size_vocab, dim_input, dim_output):\n",
    "    #y_train = np.eye(size_vocab)[y_train]\n",
    "    #y_test = np.eye(size_vocab)[y_test]\n",
    "    \n",
    "    # batch_size , sequence_length , size_encoding = 1 (> 1 if one-hot encoding)\n",
    "    \n",
    "    nb_samples = X.shape[0]\n",
    "    seq_length = X.shape[1]\n",
    "    X = np.reshape(X, (nb_samples, seq_length, dim_input))/float(size_vocab) # Normalization\n",
    "    y = np.reshape(y, (nb_samples, dim_output))\n",
    "\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "def ind_to_embedding(dataInd, dataEmbed):\n",
    "    ind2embed = {}\n",
    "    for i in range(len(dataInd)): # -1270\n",
    "        ind2embed[dataInd[i]] = dataEmbed[i]\n",
    "    \n",
    "    return ind2embed\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "class lstm_model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers, dropout):\n",
    "        super(lstm_model, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #Defining the layers\n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim, n_layers)\n",
    "        self.lstm3 = nn.LSTM(hidden_dim, hidden_dim, n_layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fully_connected = nn.Linear(hidden_dim, output_size)\n",
    "      \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        len_seq = x.size(1)\n",
    "        x = x.to(dtype=torch.float64)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        h_t = hidden[0].to(dtype=torch.float64)\n",
    "        c_t = hidden[1].to(dtype=torch.float64)\n",
    "                \n",
    "        out, (h_t, c_t) = self.lstm1(x, (h_t, c_t))\n",
    "        h_t = self.dropout(h_t)\n",
    "        out, (h_t, c_t) = self.lstm2(h_t, (h_t, c_t)) \n",
    "        h_t = self.dropout(h_t)\n",
    "        out, (h_t, c_t) = self.lstm3(h_t, (h_t, c_t)) \n",
    "     \n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fully_connected(out)\n",
    " \n",
    "        return out\n",
    "\n",
    "    \"\"\"\n",
    "    This function can be used to extract the output of the last predictions of each batch\n",
    "    \"\"\"\n",
    "    def last_output(batc_size, out):\n",
    "        lengths = [len_seq for i in range(batch_size)]\n",
    "        idx = (torch.LongTensor(lengths) - 1).view(-1, 1)\n",
    "        idx = idx.expand(len(lengths), out.size(2))\n",
    "        time_dimension = 1 # because batch_first is True so the time step is dimension 1 !\n",
    "        idx = idx.unsqueeze(time_dimension)\n",
    "        \n",
    "        if out.is_cuda:\n",
    "            idx = idx.cuda(out.data.get_device())\n",
    "        \n",
    "        out = out.gather(time_dimension, idx).squeeze(time_dimension)\n",
    "        \n",
    "        return out\n",
    " \n",
    "      \n",
    "    \"\"\"\n",
    "    Generates the hidden state and the cell state used in a lstm layers\n",
    "    \"\"\"\n",
    "    def init_hidden(self,  batch_size):\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return (torch.zeros(self.n_layers, batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.n_layers, batch_size, self.hidden_dim))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "def from_notes_to_MIDI(music_generated, name, offset):\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for pattern in music_generated:\n",
    "        \n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                \n",
    "                cn=int(current_note)\n",
    "                new_note = note.Note(cn)\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "                \n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "            \n",
    "        elif('rest' in pattern):\n",
    "            new_rest = note.Rest(pattern)\n",
    "            new_rest.offset = offset\n",
    "            new_rest.storedInstrument = instrument.Piano() #???\n",
    "            output_notes.append(new_rest)\n",
    "            \n",
    "        # pattern is a note\n",
    "        else:\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += 0.5\n",
    "    \n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    name_song = name+'.mid'\n",
    "    midi_stream.write('midi', fp=name_song)\n",
    "    print(name_song+\" downloaded succesfully !\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "def initialise_generation(dataInd, window_size, ind2note, ind2embed):\n",
    "    song_ind = np.random.randint(0, len(dataInd))\n",
    "    note_ind = np.random.randint(0+window_size, len(dataInd[song_ind])-window_size-1)\n",
    "    init_sequence = dataInd[song_ind][note_ind:note_ind+window_size]\n",
    "    \n",
    "    music_generated = list()\n",
    "    input_sequence = list()\n",
    "    for ind in init_sequence:\n",
    "        note = ind2note[ind]\n",
    "        embed = ind2embed[ind]\n",
    "        input_sequence.append(embed)\n",
    "        music_generated.append(note)\n",
    "        \n",
    "        \n",
    "    return music_generated, np.array(input_sequence)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "def generate_note(input_seq):\n",
    "    input_seq = np.reshape(input_seq, (1, input_seq.shape[0], dim_input))\n",
    "    input_seq = torch.from_numpy(input_seq)\n",
    "    input_seq.to(device)\n",
    "    \n",
    "    pred = model(input_seq)\n",
    "    prob = nn.functional.softmax(pred[-1], dim=0).data \n",
    "    note_ind = torch.max(prob, dim=0)[1].item() # [1] take indice\n",
    "    \n",
    "\n",
    "    return note_ind\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "def music_generation(model, music_generated, input_sequence, ind2note, ind2embed, nb_steps, name_song):\n",
    "    \n",
    "    for i in range(nb_steps):\n",
    "        pred_ind = generate_note(input_sequence)\n",
    "        pred_note = ind2note[pred_ind]\n",
    "        music_generated.append(pred_note)\n",
    "        pred_embed = ind2embed[pred_ind]\n",
    "        input_sequence = np.append(np.delete(input_sequence, 0, axis=0), [pred_embed], axis=0)\n",
    "\n",
    "    offset = 0.5\n",
    "    from_notes_to_MIDI(music_generated, name_song, offset)\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "def get_dataset(file, window_size, dim_input, dim_output, batch_size, split_ratio):\n",
    "    data = read_midi_dataset(file)\n",
    "    unique_note, vocab = get_vocabulary(data)\n",
    "    size_vocab = len(unique_note)\n",
    "    \n",
    "    data_embed = note_to_vec(vocab, data, size_vocab, dim_input)\n",
    "    dataInd, note2ind, ind2note = note_to_ind(unique_note, data)\n",
    "    ind2embed = ind_to_embedding(dataInd[0], data_embed[0])\n",
    "    \n",
    "    X_dataset, y_dataset = training_target_samples(data_embed, dataInd, window_size)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = split_reshape(X_dataset, y_dataset, split_ratio,\n",
    "                                                    size_vocab, dim_input, dim_output)\n",
    "                                                \n",
    "    train_data = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "    test_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
    "\n",
    "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "    test_loader = DataLoader(test_data, shuffle=True, batch_size=1, drop_last=True) #batch_size=1 for testing\n",
    "    \n",
    "    \n",
    "    return train_loader, test_loader, dataInd, ind2note, ind2embed, size_vocab\n",
    "\n",
    "\n",
    "def model_train(model, n_epochs, batch_size, train_loader, test_loader):\n",
    "    \n",
    "    # Parameters and Variables for the training\n",
    "    print_every = batch_size\n",
    "    valid_loss_min = np.Inf\n",
    "    train_accuracy = list() # to return\n",
    "    test_accuracy = list() # to return\n",
    "    save = 0\n",
    "    \n",
    "    train_loss = list()\n",
    "    val_loss = list()\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
    "        true_pred_train = 0\n",
    "        true_pred_test = 0\n",
    "        \n",
    "        epoch_loss = list()\n",
    "        epoch_val_loss = list()\n",
    "        for inputs, targets in train_loader:\n",
    "            \n",
    "            # Forward\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            pred_train = model(inputs)\n",
    "            \n",
    "            # Compute Loss and backpropagation\n",
    "            loss_train = criterion(pred_train, targets.view(-1).long())\n",
    "            epoch_loss.append(loss_train.item())\n",
    "            loss_train.backward() # Does backpropagation and calculates gradients\n",
    "            optimizer.step() # Updates the weights accordingly\n",
    "            \n",
    "            # Check if the right target has been predicted for the last input of the batch\n",
    "            prob = nn.functional.softmax(pred_train[-1], dim=0).data \n",
    "            note_ind = torch.max(prob, dim=0)[1].item() # [1] take indice\n",
    "        \n",
    "            if note_ind == targets[-1].item(): # compare to the last target of the batch\n",
    "                true_pred_train+=1\n",
    "            \n",
    "        # mean accuracy and loss accuracy of this training epoch\n",
    "        train_loss.append(np.mean(epoch_loss))\n",
    "        train_accuracy.append(true_pred_train/len(train_loader.dataset))\n",
    "        \n",
    "        model.eval()\n",
    "        for inputs, target in test_loader:\n",
    "\n",
    "            inputs, target = inputs.to(device), target.to(device)\n",
    "            pred_test = model(inputs)\n",
    "            \n",
    "            loss_test = criterion(pred_test, target.view(-1).long())\n",
    "            epoch_val_loss.append(loss_test.item())\n",
    "            \n",
    "            prob = nn.functional.softmax(pred_test[-1], dim=0).data \n",
    "            note_ind = torch.max(prob, dim=0)[1].item() # [1] take indice\n",
    "            \n",
    "            if note_ind == target[-1].item():\n",
    "                true_pred_test+=1\n",
    "        \n",
    "        # mean accuracy and loss accuracy of this testing epoch\n",
    "        val_loss.append(np.mean(epoch_val_loss))\n",
    "        test_accuracy.append(true_pred_test/len(test_loader.dataset))\n",
    "        \n",
    "        model.train()\n",
    "        print(\"Epoch: {}/{} => \".format(epoch, n_epochs),\n",
    "            \"Train Loss: {:.6f}, \".format(np.mean(epoch_loss)),\n",
    "            \"Val Loss: {:.6f}, \".format(np.mean(epoch_val_loss)),\n",
    "            \"Train accuracy: {:.6f},\".format(true_pred_train/len(train_loader.dataset)),\n",
    "            \"Test accuracy: {:.6f}\".format(true_pred_test/len(test_loader.dataset)))\n",
    "        \n",
    "        \n",
    "        save+=1\n",
    "        if save == 15:\n",
    "            save = 0\n",
    "            if np.mean(epoch_val_loss) <= valid_loss_min:\n",
    "                valid_loss_min = np.mean(epoch_val_loss)\n",
    "                torch.save(model.state_dict(), './state_dict.pt')\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}). Saving model ...'.\n",
    "                    format(valid_loss_min,np.mean(epoch_val_loss)))\n",
    "    \n",
    "    return model, train_accuracy, test_accuracy\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # PARAMETERS FOR THE DATASET.\n",
    "    file = \"/home/cj/Bureau/Master2/Q2/deep_learning/project/tf_dataset/*.mid\"\n",
    "    split_ratio = 0.90\n",
    "    dim_output = 1 \n",
    "    \n",
    "    # HYPER_PARAMETERS FOR THE DATASET.\n",
    "    window_size = 5\n",
    "    batch_size = 3\n",
    "    dim_input = 4\n",
    "    \n",
    "    train_loader, test_loader, dataInd, ind2note, ind2embed, size_vocab = get_dataset(file,\n",
    "                                                                                       window_size,\n",
    "                                                                                       dim_input,\n",
    "                                                                                       dim_output,\n",
    "                                                                                       batch_size,\n",
    "                                                                                       split_ratio)\n",
    "    \n",
    "    # Look if there is a GPU available or not. Otherwise the CPU is used.\n",
    "    is_cuda = torch.cuda.is_available()\n",
    "    if is_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"cpu\")\n",
    "\n",
    "    # PARAMETERS FOR THE MODEL USED.\n",
    "    n_epochs = 1\n",
    "    n_layers = 1\n",
    "    \n",
    "    # HYPER_PARAMETERS FOR THE MODEL USED.\n",
    "    hidden_dim = 12 \n",
    "    dropout = 0.3 \n",
    "    lr = 0.01 # no tested\n",
    "    \n",
    "    # INITIALIZATION OF THE LSTM MODEL\n",
    "    model = lstm_model(input_size=dim_input, output_size=size_vocab,\n",
    "                       hidden_dim=hidden_dim, n_layers=n_layers, dropout=dropout)\n",
    "    model = model.to(dtype=torch.float64)\n",
    "    model.to(device)\n",
    "    \n",
    "    # DEFINE LOSS AND OPTIMIZER\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "\n",
    "    # TRAIN THE MODEL\n",
    "    model, train_accuracy, test_accuracy = model_train(model.train(), n_epochs, batch_size,\n",
    "                                                       train_loader, test_loader)\n",
    "    \n",
    "    # GENERARTE MUSIC\n",
    "    nb_steps = 50 # how many words are generated\n",
    "    name_song = 'pytorch_song'\n",
    "    music_generated, input_sequence = initialise_generation(dataInd, window_size, ind2note, ind2embed)\n",
    "    music_generation(model, music_generated, input_sequence, ind2note, ind2embed, nb_steps, name_song)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('rstudio': conda)",
   "language": "python",
   "name": "python37464bitrstudioconda85139be567af4fb89143852fd1a4ee46"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
